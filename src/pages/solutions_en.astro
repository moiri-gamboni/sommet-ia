---
import Layout from '../layouts/Layout.astro';
import Container from '../components/Container.astro';
import BackgroundImage from '../components/BackgroundImage.astro';
import LanguageSwitcher from '../components/LanguageSwitcher.astro';

const title = "Solutions and Lessons Learned - AI: Taking Back Control";
const description = "Discover concrete solutions and testimonials shared by our community to regain control over AI";
---

<Layout title={title} description={description}>
	<div class="relative py-12 sm:pb-24 sm:pt-24">
		<BackgroundImage class="-bottom-14 -top-36" />
		<Container class="relative">
			<LanguageSwitcher currentLang="en" />
			<div class="mx-auto max-w-2xl lg:max-w-4xl lg:px-12">
				<h1 class="font-display text-4xl font-medium tracking-tight text-center text-blue-800 mb-12">
					Forum Results: <br>Key Solutions & Proposals
				</h1>
				<div class="mt-6 space-y-6 text-lg tracking-tight text-blue-900">
					<p>
						On February 8, 2025, the largest French-speaking conference ever organized on AI safety and ethics was held at the Learning Planet Institute in Paris. Organized by <a href="https://pauseia.fr/" class="text-blue-600 hover:underline">Pause IA</a>, this historic event brought together a hundred experts, researchers, and decision-makers on the eve of the Summit for Action on AI.
					</p>

					<p>
						As artificial intelligence development accelerates, with the emergence of models like OpenAI's O3 or DeepSeek R1, risks that were once theoretical are materializing faster than anticipated. In light of this reality, our Forum aimed to examine these risks and identify concrete solutions to address them.
					</p>

					<div class="my-8">
						<p class="mb-4">The diversity and expertise of the speakers reflect the urgency felt by the entire community:</p>
						<ul class="list-none space-y-2 pl-4">
							<li><strong>Researchers and Academics</strong>: Raja Chatila (Professor Emeritus, Sorbonne University), Le Nguyen Hoang (President, Tournesol), Otto Barten (X-Risk Observatory)</li>
							<li><strong>AI Safety Experts</strong>: Charbel-Raphael Segerie (Executive Director, CeSIA), Henri Papadatos (Managing Director, Safer AI), Adam Shimi (Control AI), Jeremy Perret (Suboptimal AI)</li>
							<li><strong>Civil Society Organizations</strong>: Anne-Sophie Simpere (Coordinator, Stop Killer Robots), Lou Welgryn (Co-President, Data For Good), Maxime Fournes (President, Pause IA), Henri-Alexis Corvol (AI Observatory)</li>
							<li><strong>Technical Experts</strong>: Marc Faddoul (Director, AI Forensics), Mathilde Cerioli (Chief Scientist, everyone.AI)</li>
							<li><strong>Analysts and Foresight Experts</strong>: Arthur Grimonpont (Reporters Without Borders), Flavien Chervet (Writer and Speaker)</li>
						</ul>
					</div>

					<div class="my-8">
						<p class="mb-4">The discussions highlighted three major findings:</p>
						<ol class="list-decimal pl-6 space-y-2">
							<li>AI risks are no longer speculative but are already materializing, from massive disinformation to automated cyberattacks.</li>
							<li>Current safety measures are dangerously inadequate in the face of accelerating AI capabilities.</li>
							<li>Concrete solutions exist, both technical and regulatory, but require immediate political mobilization.</li>
						</ol>
					</div>

					<p>
						This report synthesizes the analyses and proposals made during this day, with the aim of informing discussions at the Summit for Action on AI and encouraging a coordinated response to the challenges ahead.
					</p>

					<h2 class="text-2xl font-bold text-blue-800 mt-12 mb-6">Executive Summary</h2>
					<p>On February 8, 2025, on the eve of the Summit for Action on AI, fifteen experts, ten organizations, and a hundred participants gathered for the largest French-speaking conference ever organized on AI safety. The discussions highlighted three major findings and a set of concrete solutions.</p>

					<h3 class="text-xl font-semibold text-blue-800 mb-3">1. The Materialization of Risks</h3>
					<p>AI risks are no longer speculative - they are materializing faster than expected. From deepfakes manipulating public opinion to automated cyberattacks paralyzing critical infrastructure, and the growing loss of digital sovereignty, yesterday's theoretical concerns have become today's challenges.</p>

					<h3 class="text-xl font-semibold text-blue-800 mb-3">2. The Alarming Inadequacy of Safety Measures</h3>
					<p>Current AI safety standards are dramatically insufficient compared to those of other high-risk industries like aviation or nuclear power. While AI capabilities are experiencing exponential growth in 2025, safety measures remain embryonic.</p>

					<h3 class="text-xl font-semibold text-blue-800 mb-3">3. Concrete Solutions Within Our Reach</h3>
					<p>Solutions exist and are ready to be deployed. From proven risk management frameworks to international governance initiatives like the Conditional AI Safety Treaty, we have concrete tools to secure AI development. What is lacking today is not technical feasibility but the political will to implement these solutions before it's too late.</p>

					<p>This synthesis of the Forum's work constitutes an urgent call to action as the Summit for Action on AI begins. France has an international platform to convey a strong message: only ambitious international governance of AI can preserve our common future.</p>
				</div>
			</div>
		</Container>
	</div>
</Layout> 