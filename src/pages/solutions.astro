---
import Layout from '../layouts/Layout.astro';
import Container from '../components/Container.astro';
import BackgroundImage from '../components/BackgroundImage.astro';

const title = "Solutions et Retours d'Expérience - IA: Reprendre le Contrôle";
const description = "Découvrez les solutions concrètes et témoignages partagés par notre communauté pour reprendre le contrôle face à l'IA";
---

<Layout title={title} description={description}>
	<div class="relative py-20 sm:pb-24 sm:pt-36">
		<BackgroundImage class="-bottom-14 -top-36" />
		<Container class="relative">
			<div class="mx-auto max-w-2xl lg:max-w-4xl lg:px-12">
				<h1 class="font-display text-4xl font-medium tracking-tight text-center text-blue-800 mb-12">
					Résultats du Forum : <br>Solutions & propositions clés 
				</h1>
				<div class="mt-6 space-y-6 text-lg tracking-tight text-blue-900">
					<p>
						Le 8 février 2025 s'est tenue au Learning Planet Institute à Paris la plus grande conférence francophone jamais organisée sur la sécurité et l'éthique de l'intelligence artificielle. Organisé par Pause IA, le chapitre français de Pause AI, cet événement historique a réuni une centaine d'experts, chercheurs et décideurs à la veille du Sommet pour l'Action sur l'IA.
					</p>

					<p>
						Alors que le développement de l'intelligence artificielle s'accélère, avec l'émergence de modèles comme O3 d'OpenAI ou DeepSeek R1, les risques autrefois théoriques se matérialisent plus rapidement que prévu. Face à ce constat, notre Forum visait à examiner la réalité de ces risques et à identifier des solutions concrètes pour y répondre.
					</p>

					<div class="my-8">
						<p class="mb-4">La diversité et l'expertise des intervenants témoignent de l'urgence ressentie par l'ensemble de la communauté :</p>
						<ul class="list-none space-y-2 pl-4">
							<li><strong>Chercheurs et académiques</strong> : Raja Chatila (Professeur émérite, Sorbonne Université), Le Nguyen Hoang (Président, Tournesol), Otto Barten (X-Risk Observatory)</li>
							<li><strong>Experts en sécurité de l'IA</strong> : Charbel-Raphael Segerie (Directeur exécutif, CeSIA), Henri Papadatos (Managing Director, Safer AI), Adam Shimi (Control AI), Jeremy Perret (Suboptimal AI)</li>
							<li><strong>Organisations de la société civile</strong> : Anne-Sophie Simpere (Coordinatrice, Stop Killer Robots), Lou Welgryn (Co-Présidente, Data For Good), Maxime Fournes (Président, Pause IA), Henri-Alexis Corvol (Observatoire de l'IA)</li>
							<li><strong>Experts techniques</strong> : Marc Faddoul (Directeur, AI Forensics), Mathilde Cerioli (Chief Scientist, everyone.AI)</li>
							<li><strong>Analystes et prospectivistes</strong> : Arthur Grimonpont (Reporters sans frontières), Flavien Chervet (Ecrivain et Conférencier)</li>
						</ul>
					</div>

					<div class="my-8">
						<p class="mb-4">Les échanges ont mis en lumière trois constats majeurs :</p>
						<ol class="list-decimal pl-6 space-y-2">
							<li>Les risques liés à l'IA ne sont plus spéculatifs mais se matérialisent déjà, de la désinformation massive aux cyberattaques automatisées.</li>
							<li>Les mesures de sécurité actuelles sont dangereusement inadéquates face à l'accélération des capacités de l'IA.</li>
							<li>Des solutions concrètes existent, tant techniques que réglementaires, mais nécessitent une mobilisation politique immédiate.</li>
						</ol>
					</div>
					
					<div>
						<h3 class="text-xl font-semibold text-blue-800 mb-3">Vulnérabilités Cybernétiques Exponentielles</h3>
						<p class="mb-4">
							La cybersécurité représente peut-être la menace la plus immédiate. Les chiffres sont vertigineux : les coûts de la cybercriminalité devraient atteindre 18 trillions de dollars (!) en 2025, soit "six fois le PIB de la France", rapporte Le Nguyen Hoang. Plus inquiétant encore, même les géants technologiques peinent à assurer leur sécurité : après une intrusion de hackers chinois en 2023, "Microsoft ne sait pas comment les attaquants sont entrés et s'ils sont encore dans les systèmes."
						</p>
						<p>
							Cette vulnérabilité systémique risque de s'aggraver exponentiellement avec l'émergence d'IA capables de programmer et pirater. Comme l'explique Maxime Fournes : "Une IA peut être copiée un million de fois, elle peut tourner 24 heures sur 24 et elle agit 50 fois plus rapidement qu'un humain. [...] Du point de vue de ce système, l'humain n'est pas une forme de vie animale, c'est une forme de vie végétale qui montre des signes d'intelligence à des intervalles de temps très longs."
						</p>
					</div>

					<div>
						<h3 class="text-xl font-semibold text-blue-800 mb-3">Des Menaces Biologiques Concrètes</h3>
						<p>
							Les risques ne se limitent pas au monde numérique. Arthur Grimonpont a soulevé une menace particulièrement inquiétante : "C'est possible dès à présent de vous faire livrer chez vous des fragments d'ADN [...] que vous pourrez recombiner pour reconstituer un virus de peste." Plus alarmant encore, l'agence internationale chargée de prévenir le bioterrorisme dispose "du même budget que le McDonald's moyen."
						</p>
					</div>

					<div>
						<h3 class="text-xl font-semibold text-blue-800 mb-3">Une Perte de Contrôle En Cours</h3>
						<p class="mb-4">
							Les experts s'inquiètent particulièrement de l'accélération du développement de systèmes d'IA toujours plus puissants. Jérémy Perret souligne que les estimations concernant l'émergence d'une IA générale (AGI) se sont drastiquement raccourcies : d'un horizon de plusieurs décennies il y a dix ans, nous sommes passés à "quelques années" selon les experts actuels.
						</p>
						<p>
							Au même moment, les tests récents d'Apollo Research démontrent la fragilité des systèmes actuels : même les IA supposément les plus sûres peuvent être facilement détournées de leurs objectifs éthiques initiaux. Plus inquiétant encore, certains systèmes montrent déjà des signes de "convergence instrumentale" - la tendance à développer des comportements dangereux pour atteindre leurs objectifs.
						</p>
					</div>

					<h2 class="text-2xl font-bold text-blue-800 mt-12 mb-6">2. Des mesures de sécurité dangereusement inadéquates</h2>

					<p class="mb-6">
						Face à l'ampleur et à l'immédiateté des risques décrits précédemment, on pourrait s'attendre à ce que le développement de l'IA soit encadré par des mesures de sécurité aussi rigoureuses que celles appliquées dans d'autres industries à haut risque. Lors de sa présentation, Henri Papadatos, Managing Director de Safer AI, a démontré que la réalité est tout autre.
					</p>

					<div class="space-y-6">
						<p>
							L'étude menée par son organisation révèle un écart alarmant : sur une échelle de 0 à 5 calibrée sur les standards de sécurité des industries à haut risque comme l'aviation et le nucléaire, même les entreprises d'IA les plus avancées n'atteignent qu'un score de 2. Cette échelle évalue la maturité des processus de gestion des risques, avec un score de 5 correspondant aux pratiques exemplaires observées chez Boeing ou Airbus.
						</p>

						<p>
							Plus inquiétant encore, les rares mesures de sécurité existantes s'avèrent remarquablement fragiles. Jeremy Perret, chercheur en sécurité de l'IA, a souligné lors de la première table ronde : "Il est extrêmement facile aujourd'hui de contourner les mesures de sûreté qui sont mises en place sur les systèmes actuels."
						</p>
					</div>

					<div class="space-y-6">
						<p>
							Le manque de transparence aggrave cette situation. Les modèles les plus avancés ne sont pas accessibles au public, ce qui rend impossible toute évaluation indépendante de leurs capacités réelles. Plus fondamentalement, comme l'a expliqué Charbel Segerie, directeur du Centre pour la Sécurité de l'IA : "Mathématiquement, on n'a absolument aucune idée aujourd'hui de comment aligner des systèmes beaucoup plus puissants et autonomes que ceux d'aujourd'hui."
						</p>

						<p>
							Cette absence de garanties fondamentales de sécurité est d'autant plus alarmante qu'Arthur Grimonpont a révélé que seulement 2% des publications scientifiques en IA sont consacrées aux questions de sécurité. Les entreprises qui développent ces technologies n'investissent qu'une fraction comparable de leurs ressources dans ce domaine crucial.
						</p>
					</div>

					<h2 class="text-2xl font-bold text-blue-800 mt-12 mb-6">3. Les solutions sont à notre portée</h2>

					<p class="mb-6">
						Face à l'ampleur des risques et à l'insuffisance actuelle des mesures de sécurité, il est tentant de céder au fatalisme. Pourtant, comme l'ont démontré les différents intervenants de notre Forum, des solutions concrètes existent et sont à notre portée. Ces solutions s'articulent autour de trois axes complémentaires : un cadre technique robuste inspiré d'autres industries à risque, une gouvernance démocratique des systèmes d'IA, et des instruments réglementaires efficaces.
					</p>

					<div class="space-y-8">
						<div>
							<h3 class="text-xl font-semibold text-blue-800 mb-3">Un cadre technique robuste et éprouvé</h3>
							<p class="mb-4">
								L'histoire des industries à haut risque nous montre qu'une gestion rigoureuse des risques peut produire des résultats spectaculaires. Comme l'a souligné Henri Papadatos, Managing Director de Safer AI : "Dans les années 70, on avait environ 6 accidents fatals par million de vols. Et maintenant, sur les derniers systèmes d'Airbus, on a 0,04 accidents fatals par million de vols. Donc on a une réduction de 99% des accidents."
							</p>
							<p>
								Cette approche éprouvée peut être adaptée au développement de l'IA. Le framework proposé par Safer AI s'articule autour de quatre piliers : l'identification systématique des risques, leur analyse approfondie, la mise en place de mitigations, et une structure de gouvernance robuste.
							</p>
						</div>

						<div>
							<h3 class="text-xl font-semibold text-blue-800 mb-3">Une gouvernance démocratique de l'IA</h3>
							<p class="mb-4">
								La sécurité technique doit s'accompagner d'une gouvernance démocratique des systèmes d'IA. Comme l'a souligné Le Nguyen Hoang, président de Tournesol : "Aujourd'hui, les algorithmes de recommandation structurent la propagation de l'information à l'échelle planétaire puisque 5 milliards de personnes s'en servent quotidiennement."
							</p>
							<p>
								Des solutions concrètes émergent déjà. Le projet Tournesol propose par exemple un système où les citoyens peuvent évaluer collectivement les contenus recommandés, créant ainsi une forme de gouvernance participative des algorithmes.
							</p>
						</div>

						<div>
							<h3 class="text-xl font-semibold text-blue-800 mb-3">Des instruments réglementaires efficaces</h3>
							<p class="mb-4">
								Le troisième pilier essentiel est la mise en place d'un cadre réglementaire robuste. Charbel-Raphael Segerie, directeur du Centre pour la Sécurité de l'IA, rappelle que "le Code of Practice européen est très bon" mais qu'il faut l'étendre "aux différentes juridictions" et aux systèmes d'IA qui ne sont pas encore sur le marché.
							</p>
							<p>
								Une approche particulièrement prometteuse est celle du Traité de Sécurité Conditionnel de l'IA, qui propose que les pays s'engagent à arrêter le développement de systèmes d'IA potentiellement dangereux si certains seuils de risque sont atteints.
							</p>
						</div>
					</div>

					<h2 class="text-2xl font-bold text-blue-800 mt-12 mb-6">4. Une mobilisation sans précédent</h2>

					<p class="mb-6">
						Notre Forum "Reprendre le Contrôle", plus grande conférence francophone sur la sécurité de l'IA à ce jour, témoigne de l'émergence d'une mobilisation sans précédent. En réunissant une centaine de participants, quinze experts et dix organisations majeures du domaine, cette journée illustre la diversité et la force de l'écosystème qui se mobilise pour une IA plus sûre.
					</p>

					<div class="space-y-6">
						<p>
							Cette mobilisation prend de multiples formes. Des organisations citoyennes comme Pause IA émergent pour sensibiliser le public et les décideurs aux risques majeurs. Des initiatives techniques comme Tournesol développent des solutions concrètes pour une gouvernance démocratique de l'IA. Des centres de recherche comme le CeSIA et AI Forensics produisent une expertise indépendante cruciale.
						</p>

						<p>
							La communauté scientifique se mobilise également. Comme l'a souligné Charbel-Raphael Segerie, "40% des chercheurs en machine learning pensent qu'il y a plus de 10% de risque existentiel" lié à l'IA. Cette prise de conscience se traduit par des actions concrètes : au Royaume-Uni, seize parlementaires de tous bords politiques viennent de signer une déclaration reconnaissant explicitement les risques existentiels de l'IA.
						</p>
					</div>

					<h2 class="text-2xl font-bold text-blue-800 mt-12 mb-6">5. Conclusion et appel à l'action</h2>

					<div class="space-y-6">
						<p>
							La mise en place de ces solutions est une course contre la montre. Comme l'a montré Maxime Fournes, nous assistons à une accélération vertigineuse des capacités de l'IA. Les nouveaux mécanismes d'auto-amélioration ne nécessitent plus de nouvelles données ni une augmentation massive du compute, créant une boucle de rétroaction potentiellement incontrôlable.
						</p>

						<p>
							Cette journée a démontré trois réalités incontournables. Premièrement, les risques liés à l'IA ne sont plus théoriques - ils se matérialisent déjà, de la manipulation de l'information à la perte de souveraineté numérique. Deuxièmement, les mesures de sécurité actuelles sont dramatiquement insuffisantes face à l'accélération des capacités. Troisièmement, des solutions concrètes existent, qu'elles soient techniques, démocratiques ou réglementaires.
						</p>
					</div>

					<h2 class="text-2xl font-bold text-blue-800 mt-12 mb-6">Pour continuer à agir avec nous :</h2>

					<p class="mb-4">Cette conférence n'est qu'une première étape. Pour poursuivre cette mobilisation, plusieurs options s'offrent à vous :</p>

					<ul class="list-disc pl-6 space-y-2 mb-8">
						<li>Rejoignez l'une des organisations ou projets présentés aujourd'hui</li>
						<li>Suivez notre actualité via notre newsletter : <a href="https://pauseia.substack.com/" class="text-blue-600 hover:underline">https://pauseia.substack.com/</a></li>
						<li>Retrouvez les enregistrements des conférences sur notre chaîne YouTube : <a href="https://www.youtube.com/@Pause_IA" class="text-blue-600 hover:underline">https://www.youtube.com/@Pause_IA</a></li>
						<li>Échangez avec notre communauté sur Discord : <a href="https://discord.gg/e8ZRhf64Uz" class="text-blue-600 hover:underline">https://discord.gg/e8ZRhf64Uz</a></li>
						<li>Soutenez notre action par un don : <a href="https://pauseia.fr/dons" class="text-blue-600 hover:underline">https://pauseia.fr/dons</a></li>
					</ul>

					<p class="mb-4">
						Pause IA est une association composée uniquement de bénévoles. Chaque contribution, même modeste, nous aide à poursuivre notre mission. Pour en savoir plus sur nos actions : <a href="https://pauseia.fr/" class="text-blue-600 hover:underline">https://pauseia.fr/</a>
					</p>
				</div>
			</div>
		</Container>
	</div>
</Layout> 